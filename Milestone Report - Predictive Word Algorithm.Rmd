---
title: 'Milestone Report: Predictive Word Algorithm'
author: "Victor Lacerda"
date: "7/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The aim of this project is to create a predictive word algorithm and deploy this into a ShinyApp. The provided [dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) was provided by SwiftKey in cooperation with Johns Hopkins University on Coursera and consists of Twitter, Blogs, and News extracts in English, German, Finish and Russian.

## Libraries Used 

```{r libraries, echo=T, results='hide', message=F, warning=F}
library(stringr); library(tm); library(dplyr); library(tibble); library(reshape); library(caret); library(RWeka) 
library(LaF); library(jsonlite); library(purrr); library(data.table); library(tidyr); library(ggplot2)
library(textcat); library(tidytext)
```

## Data Processing

### 1. Importing Data

```{r importing}
if (! file.exists("Coursera-SwiftKey.zip")) {
        download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip", mode = "wb")
        unzip("Coursera-SwiftKey.zip")
}

files <- list.files(pattern = "\\.txt$", recursive = T)

set.seed(133)
for (i in 1:length(files)) {
        assign(files[i], sample_lines(files[i], 1000))
}

dat_all <- c(mget(unlist(files)))

rm(list = files)
```

During the import we also sample the files by use of *sample_lines*

### 2. Cleaning Data 

```{r cleaning}
dat_all <- dat_all %>%
        map(., as.data.frame) %>%
        rbindlist(., fill = T, idcol = T) %>%
        dplyr::rename(., doc_id = .id, text = `.x[[i]]`) %>%
        mutate(., doc_id = as.factor(sub(".*\\_ *(.*?) *.txt*", "\\1", doc_id)),
               language = ifelse(str_extract(doc_id, "[^.]+") == "US", "EN", str_extract(doc_id, "[^.]+")))

lang <- unique(gsub("(?:.*/){2}([^_]+)_.*", "\\1", files))
token <- function(x) {
        NGramTokenizer(x, Weka_control(min = 1, max = 4))
}

for (i in 1:length(lang)) {
        assign(paste("corpus", lang[i], sep = "_"),
               dat_all %>%
                       subset(., .$language == toupper(lang[i])) %>%
                       group_by(., doc_id) %>%
                       summarise(., text = Reduce(paste, text)) %>%
                       as.data.frame(.) %>%
                       DataframeSource(.) %>%
                       VCorpus(., readerControl = list(language = lang[i])) %>%
                       tm_map(., content_transformer(tolower)) %>%
                       tm_map(., content_transformer(removePunctuation)) %>%
                       tm_map(., content_transformer(removeNumbers)) %>%
                       tm_map(., content_transformer(stripWhitespace)) %>%
                       tm_map(., content_transformer(PlainTextDocument)) %>%
                       tm_map(., removeWords, stopwords(kind = lang[i])) %>%
                       tm_map(., stemDocument, lang[i]) %>%
                       DocumentTermMatrix(., control = list(tokenize = token)) %>%
                       removeSparseTerms(., 0.99) %>%
                       as.matrix(.) %>%
                       as.data.frame(.) %>%
                       rownames_to_column(., var = "Doc") %>%
                       gather(., Word, Frequency, names(.)[-1]) %>%
                       mutate(., Language = ifelse(str_extract(Doc, "[^.]+") == "US", "EN", str_extract(Doc, "[^.]+"))) %>%
                       mutate_if(., is.character, as.factor) %>%
                       group_by(., Language, Word) %>%
                       summarise(., Frequency = sum(Frequency)) %>%
                       data.frame(.)
        )
}

dat_all <- reshape::merge_all(list(corpus_de, corpus_en, corpus_fi, corpus_ru)) %>%
        mutate(., N_gram = str_count(Word, "\\S+"))

rm(corpus_de, corpus_en, corpus_fi, corpus_ru)
```

During the cleaning we perform the following text-manipulations:

+ Lowercase
+ Remove punctuations
+ Remove numbers
+ Stripe white spaces

In addition, to prepare the data to be used in the predictive algorithm we:

+ Remove stopwords in the texts language to avoid predicting words like: "the", "it", "and" etc.
+ Stem the data to reduce words to unify accross sentences
+ Remove sparse words (words with low frequency) from the data as they won't play a high importance in the algorithm

### 3. Exploratory Analysis 

```{r exploratory}
gram1.plot <- dat_all %>%
        subset(., N_gram == 1) %>% 
        group_by(., Language) %>%
        top_n(., 10, Frequency) %>%
        ggplot(., aes(x = reorder_within(Word, -Frequency, Language), y = Frequency, fill = Language)) + 
        geom_bar(stat="identity", position="dodge") + facet_wrap(~ Language, ncol = 4, scales='free_x') + theme_bw() + 
        labs(title = "1-gram: Top 10 Words by Language", x = "Words") + scale_x_reordered() + 
        theme(axis.text.x = element_text(angle = 45,hjust = 1))

gram2.plot <- dat_all %>%
        subset(., N_gram == 2) %>% 
        group_by(., Language) %>%
        top_n(., 10, Frequency) %>%
        ggplot(., aes(x = reorder_within(Word, -Frequency, Language), y = Frequency, fill = Language)) + 
        geom_bar(stat="identity", position="dodge") + facet_wrap(~ Language, ncol = 4, scales='free_x') + theme_bw() + 
        labs(title = "2-gram: Top 10 Words by Language", x = "Words") + scale_x_reordered() + 
        theme(axis.text.x = element_text(angle = 45,hjust = 1))

gram3.plot <- dat_all %>%
        subset(., N_gram == 3) %>% 
        group_by(., Language) %>%
        top_n(., 10, Frequency) %>%
        ggplot(., aes(x = reorder_within(Word, -Frequency, Language), y = Frequency, fill = Language)) + 
        geom_bar(stat="identity", position="dodge") + facet_wrap(~ Language, ncol = 4, scales='free_x') + theme_bw() + 
        labs(title = "3-gram: Top 10 Words by Language", x = "Words") + scale_x_reordered() + 
        theme(axis.text.x = element_text(angle = 45,hjust = 1))

gram4.plot <- dat_all %>%
        subset(., N_gram == 4) %>% 
        group_by(., Language) %>%
        top_n(., 10, Frequency) %>%
        ggplot(., aes(x = reorder_within(Word, -Frequency, Language), y = Frequency, fill = Language)) + 
        geom_bar(stat="identity", position="dodge") + facet_wrap(~ Language, ncol = 4, scales='free_x') + theme_bw() + 
        labs(title = "4-gram: Top 10 Words by Language", x = "Words") + scale_x_reordered() + 
        theme(axis.text.x = element_text(angle = 45,hjust = 1))
```

```{r plot1}
gram1.plot
```

```{r plot2}
gram2.plot
```
